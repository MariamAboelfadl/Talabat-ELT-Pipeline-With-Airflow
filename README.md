# ğŸ´ Talabat Data Pipeline

![Airflow](https://img.shields.io/badge/Airflow-2.x-green?style=flat&logo=apache-airflow) 
![Postgres](https://img.shields.io/badge/Postgres-13-blue?style=flat&logo=postgresql) 
![BigQuery](https://img.shields.io/badge/BigQuery-GCP-orange?style=flat&logo=google-cloud) 
![Python](https://img.shields.io/badge/Python-3.9+-yellow?style=flat&logo=python)


---

## âœ¨ Introduction
Welcome to the **Talabat Data Pipeline Project** ğŸš€  

This repository demonstrates how to build a **production-style ELT pipeline** that moves data from an **operational database (Postgres)** into a **cloud data warehouse (BigQuery)**, applies **transformations**, and prepares data for **analytics & reporting**.  

Itâ€™s designed as a showcase of **Data Engineering best practices**: modular Airflow DAGs, SQL-driven transformations, and orchestrated workflows.

---

## âš™ï¸ Tech Stack
| Tool/Service       | Role |
|--------------------|------|
| ğŸ˜ **Postgres**    | Source database with mock transactional data |
| â˜ **GCS**          | Staging layer for raw extracts |
| ğŸ“Š **BigQuery**    | Data warehouse (Dimensions & Facts) |
| ğŸ”„ **Airflow**     | Orchestrator for all DAGs |
| ğŸ³ **Docker**      | Local environment setup |
| ğŸ“œ **SQL**         | Schema creation & transformations |

---

## ğŸ—‚ Repository Structure
```
dags/
â”‚â”€â”€ db_pipeline.py              # Create Postgres tables + insert mock data
â”‚â”€â”€ postgres_to_bq.py           # Extract Postgres â†’ Load into BigQuery
â”‚â”€â”€ transformations_pipeline.py # Transform raw â†’ dims & facts
â”‚â”€â”€ master_dag.py               # Orchestrates all DAGs
sqlscript/
â”‚â”€â”€ talabat_create_tables.sql
â”‚â”€â”€ talabat_insert_values.sql
â”‚â”€â”€ dim_*.sql
â”‚â”€â”€ fact_*.sql
docker-compose.yaml
requirements.txt
README.md
```

---

## ğŸš€ Pipeline Overview

### 1ï¸âƒ£ Database Creation (`db_pipeline.py`)
- Creates Talabat schema in Postgres  
- Inserts **mock orders, customers, restaurants,and so on**  

---

### 2ï¸âƒ£ Data Movement (`postgres_to_bq.py`)
- Extracts data from Postgres  
- Stages CSVs in **GCS**  
- Loads into BigQuery **raw tables**  

---

### 3ï¸âƒ£ Transformations (`transformations_pipeline.py`)
- Runs all `dim_*.sql` â†’ **Dimensions**  
- Runs all `fact_*.sql` â†’ **Facts**  
- Organized with **TaskGroups**  
- Order enforced: **Dims â†’ Facts**  

---

### 4ï¸âƒ£ Orchestration (`master_dag.py`)
- Triggers DAGs **sequentially**:  
  1. `db_creation_dag`  
  2. `postgres_to_bq`  
  3. `talabat_transformations`  

---

## ğŸ› ï¸ Setup & Run

### 1. Clone Repository
```bash
git clone https://github.com/MariamAboelfadl/Talabat-ELT-Pipeline-With-Airflow.git
cd Talabat-ELT-Pipeline-With-Airflow
```

### 2. Install Requirements
```bash
pip install -r requirements.txt
```

### 3. Start Airflow
```bash
docker-compose up -d
```

### 4. Access Airflow UI
- URL â†’ `http://localhost:8080`  
- Credentials â†’ configured in `docker-compose.yaml`

---

## ğŸ“ˆ Key Learnings
- Build an **end-to-end ELT pipeline**  
- Connect Airflow with **Postgres, GCS, BigQuery**  
- Structure DAGs with **TaskGroups & modular SQL & Master Dag**  
- Transform raw â†’ **analytics-star schema model**  

---

## ğŸš§ Future Enhancements
- ğŸ” Add **data quality checks** (row counts, anomalies)  
- ğŸ— Integrate **dbt** for modular transformations   
- ğŸ“¡ Add **monitoring & alerting** (Airflow SLA)  

